#!/usr/bin/env python3
"""
run_optimized_experiment.py - åŸºäºç­–ç•¥åˆ†æç»“æœçš„ä¼˜åŒ–å®éªŒ

ä½¿ç”¨æœ€ä½³è®­ç»ƒç­–ç•¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼š
- ç­–ç•¥ï¼šå¤§æ‰¹é‡+ä½å­¦ä¹ ç‡
- é…ç½®ï¼šoptimizer=adam, lr=0.0003, batch_size=128, weight_decay=0.0001
- é¢„æœŸæ€§èƒ½ï¼šæµ‹è¯•å‡†ç¡®ç‡ ~36.93%
"""

import os
import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
import numpy as np
import json
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.models.improved_msac_t import ImprovedMSAC_T
from src.data.radioml_dataloader import RadioMLDataLoader
from src.training.optimized_trainer import OptimizedTrainer
from src.utils.config import TrainingConfig, ModelConfig
from src.evaluation.evaluation import evaluate_model


def load_optimized_data(data_subset_ratio: float = 0.2):
    """
    åŠ è½½ä¼˜åŒ–åçš„æ•°æ®é›†
    
    Args:
        data_subset_ratio: æ•°æ®å­é›†æ¯”ä¾‹ï¼Œç”¨äºå¿«é€ŸéªŒè¯
    """
    print(f"ğŸ”„ åŠ è½½æ•°æ®é›† (ä½¿ç”¨ {data_subset_ratio*100:.1f}% æ•°æ®)...")
    
    # æ•°æ®é›†è·¯å¾„
    dataset_path = "dataset/RadioML 2018.01A/GOLD_XYZ_OSC.0001_1024.hdf5"
    
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
    
    # åŠ è½½æ•°æ®
    dataloader = RadioMLDataLoader(dataset_path)
    
    # è·å–æ•°æ®å­é›†
    train_data, train_labels, val_data, val_labels, test_data, test_labels = dataloader.load_data()
    
    # å¦‚æœä½¿ç”¨æ•°æ®å­é›†
    if data_subset_ratio < 1.0:
        train_size = int(len(train_data) * data_subset_ratio)
        val_size = int(len(val_data) * data_subset_ratio)
        test_size = int(len(test_data) * data_subset_ratio)
        
        # éšæœºé‡‡æ ·
        train_indices = np.random.choice(len(train_data), train_size, replace=False)
        val_indices = np.random.choice(len(val_data), val_size, replace=False)
        test_indices = np.random.choice(len(test_data), test_size, replace=False)
        
        train_data = train_data[train_indices]
        train_labels = train_labels[train_indices]
        val_data = val_data[val_indices]
        val_labels = val_labels[val_indices]
        test_data = test_data[test_indices]
        test_labels = test_labels[test_indices]
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
    
    return train_data, train_labels, val_data, val_labels, test_data, test_labels


def create_optimized_dataloaders(train_data, train_labels, val_data, val_labels, test_data, test_labels):
    """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
    
    # ä½¿ç”¨æœ€ä½³ç­–ç•¥çš„æ‰¹é‡å¤§å°
    batch_size = 128
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(train_data),
        torch.LongTensor(train_labels)
    )
    val_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(val_data),
        torch.LongTensor(val_labels)
    )
    test_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(test_data),
        torch.LongTensor(test_labels)
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ (batch_size={batch_size})")
    
    return train_loader, val_loader, test_loader


def create_optimized_model():
    """åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹"""
    print("ğŸ”„ åˆ›å»ºä¼˜åŒ–æ¨¡å‹...")
    
    # æ¨¡å‹é…ç½®
    model_config = ModelConfig()
    
    # åŸºäºç­–ç•¥åˆ†æç»“æœä¼˜åŒ–æ¨¡å‹å‚æ•°
    model_config.feature_dim = 256
    model_config.num_heads = 8
    model_config.attention_dropout = 0.1
    model_config.dropout = 0.3  # é€‚åº¦é™ä½dropoutä»¥é…åˆå¤§æ‰¹é‡è®­ç»ƒ
    
    # åˆ›å»ºæ¨¡å‹
    model = ImprovedMSAC_T(
        num_classes=model_config.num_classes,
        feature_dim=model_config.feature_dim,
        num_heads=model_config.num_heads,
        dropout=model_config.dropout
    )
    
    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ:")
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return model


def run_optimized_experiment(data_subset_ratio: float = 0.2, epochs: int = 50):
    """
    è¿è¡Œä¼˜åŒ–å®éªŒ
    
    Args:
        data_subset_ratio: æ•°æ®å­é›†æ¯”ä¾‹
        epochs: è®­ç»ƒè½®æ•°
    """
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–å®éªŒ")
    print("=" * 60)
    print("åŸºäºç­–ç•¥åˆ†æç»“æœçš„æœ€ä½³é…ç½®:")
    print("- ç­–ç•¥: å¤§æ‰¹é‡+ä½å­¦ä¹ ç‡")
    print("- ä¼˜åŒ–å™¨: Adam")
    print("- å­¦ä¹ ç‡: 0.0003")
    print("- æ‰¹é‡å¤§å°: 128")
    print("- æƒé‡è¡°å‡: 0.0001")
    print("- é¢„æœŸæµ‹è¯•å‡†ç¡®ç‡: ~36.93%")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # 1. åŠ è½½æ•°æ®
        train_data, train_labels, val_data, val_labels, test_data, test_labels = load_optimized_data(data_subset_ratio)
        
        # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = create_optimized_dataloaders(
            train_data, train_labels, val_data, val_labels, test_data, test_labels
        )
        
        # 3. åˆ›å»ºæ¨¡å‹
        model = create_optimized_model()
        
        # 4. åˆ›å»ºä¼˜åŒ–è®­ç»ƒå™¨
        print("ğŸ”„ åˆ›å»ºä¼˜åŒ–è®­ç»ƒå™¨...")
        config = TrainingConfig()
        config.epochs = epochs
        
        trainer = OptimizedTrainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            test_loader=test_loader,
            config=config,
            device=device,
            experiment_name='optimized_msac_t'
        )
        
        print("âœ… è®­ç»ƒå™¨åˆ›å»ºå®Œæˆ")
        
        # 5. å¼€å§‹è®­ç»ƒ
        print("\nğŸ¯ å¼€å§‹ä¼˜åŒ–è®­ç»ƒ...")
        start_time = time.time()
        
        results = trainer.train()
        
        training_time = time.time() - start_time
        
        # 6. è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ‰ ä¼˜åŒ–å®éªŒå®Œæˆ!")
        print("=" * 60)
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {results['best_val_acc']:.4f} ({results['best_val_acc']:.2%})")
        print(f"   æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {results['best_test_acc']:.4f} ({results['best_test_acc']:.2%})")
        print(f"   è®­ç»ƒè½®æ•°: {results['total_epochs']}")
        print(f"   è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’ ({training_time/60:.1f}åˆ†é’Ÿ)")
        
        # 7. ä¸ç­–ç•¥åˆ†æç»“æœå¯¹æ¯”
        expected_acc = 0.3693
        actual_acc = results['best_test_acc'] / 100.0
        improvement = (actual_acc - expected_acc) / expected_acc * 100
        
        print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
        print(f"   ç­–ç•¥åˆ†æé¢„æœŸ: {expected_acc:.4f} ({expected_acc:.2%})")
        print(f"   å®é™…æµ‹è¯•ç»“æœ: {actual_acc:.4f} ({actual_acc:.2%})")
        print(f"   æ€§èƒ½å˜åŒ–: {improvement:+.1f}%")
        
        # 8. ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_results = {
            'experiment_type': 'optimized_training',
            'strategy': 'å¤§æ‰¹é‡+ä½å­¦ä¹ ç‡',
            'config': {
                'optimizer': 'adam',
                'learning_rate': 0.0003,
                'batch_size': 128,
                'weight_decay': 0.0001,
                'epochs': epochs,
                'data_subset_ratio': data_subset_ratio
            },
            'results': results,
            'performance_comparison': {
                'expected_accuracy': expected_acc,
                'actual_accuracy': actual_acc,
                'improvement_percentage': improvement
            },
            'training_time_seconds': training_time,
            'device': str(device)
        }
        
        results_file = 'optimized_experiment_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        return detailed_results
        
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è¿è¡ŒåŸºäºç­–ç•¥åˆ†æçš„ä¼˜åŒ–å®éªŒ')
    parser.add_argument('--data-ratio', type=float, default=0.2,
                        help='æ•°æ®å­é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    parser.add_argument('--epochs', type=int, default=50,
                        help='è®­ç»ƒè½®æ•° (é»˜è®¤: 50)')
    parser.add_argument('--full-data', action='store_true',
                        help='ä½¿ç”¨å®Œæ•´æ•°æ®é›†')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ•°æ®æ¯”ä¾‹
    data_ratio = 1.0 if args.full_data else args.data_ratio
    
    print(f"å‚æ•°è®¾ç½®:")
    print(f"- æ•°æ®æ¯”ä¾‹: {data_ratio*100:.1f}%")
    print(f"- è®­ç»ƒè½®æ•°: {args.epochs}")
    print()
    
    # è¿è¡Œå®éªŒ
    results = run_optimized_experiment(
        data_subset_ratio=data_ratio,
        epochs=args.epochs
    )
    
    if results:
        print("\nâœ… å®éªŒæˆåŠŸå®Œæˆ!")
    else:
        print("\nâŒ å®éªŒå¤±è´¥!")
        sys.exit(1)


if __name__ == "__main__":
    main()